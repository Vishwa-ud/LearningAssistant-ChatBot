{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook loads the local Mistral-7B model, performs retrieval\n",
    "from FAISS, and runs a RAG pipeline to answer user questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "MODEL_PATH = \"../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "VECTOR_INDEX_PATH = \"../vectorstore/ctse_faiss.index\"\n",
    "CHUNKS_PATH = \"../vectorstore/chunks.txt\"\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# --- Load LLM ---\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6, n_gpu_layers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load vector index and chunks ---\n",
    "index = faiss.read_index(VECTOR_INDEX_PATH)\n",
    "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "    chunks = f.read().split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG Pipeline ---\n",
    "def retrieve_chunks(query, k=TOP_K):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    query_vec = embedder.encode([query])\n",
    "    D, I = index.search(np.array(query_vec).astype('float32'), k)\n",
    "    print(\"Retrieved indices:\", I)\n",
    "    print(\"Retrieved distances:\", D)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def generate_answer(query):\n",
    "    context_chunks = retrieve_chunks(query)\n",
    "    context = \"\\n---\\n\".join(context_chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant helping students learn about Current Trends in Software Engineering (CTSE).\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, stop=[\"\\n\\n\"])\n",
    "    return response['choices'][0]['text'].strip(), context_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 2\n",
      "FAISS index dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Chunks:\", len(chunks))\n",
    "print(\"FAISS index dimensions:\", index.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Education\\LearningAssistant-ChatBot\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved indices: [[ 1  0 -1]]\n",
      "Retrieved distances: [[6.1679852e-01 6.3236934e-01 3.4028235e+38]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time = 1103116.47 ms\n",
      "llama_perf_context_print: prompt eval time = 1103078.44 ms /  1779 tokens (  620.06 ms per token,     1.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22290.75 ms /    32 runs   (  696.59 ms per token,     1.44 tokens per second)\n",
      "llama_perf_context_print:       total time = 1127703.61 ms /  1811 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " DevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity.\n",
      "\n",
      "Source Chunks:\n",
      " ['requests (MRs) and CI/CD to further unify software development and infrastructure operations. GitOps incorporates managing both infrastructure and applications as code. • Cloud Infrastructure - Cloud provides more flexibility, scalability and toolsets for organizations to implement DevOps culture and practices. Serverless architecture in cloud brings down the efforts of DevOps teams as it eliminates server management operations. • Continuous Monitoring, Logging and Alerting - Organizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product’s end user. Combined with real time alerts organizations can do a real time analysis on the application status. DevOps Tools and Technologies Beyond DevOps \"the practice of integrating security into a continuous integration, continuous delivery, and continuous deployment pipeline\" DevSecOps Idea of moving Security in the early stages of the SDLC pipeline SRE (Site Reliability Engineering) ⚬Not competing with DevOps ⚬Think that Class SRE implements Interface DevOps ⚬SRE is a part of the DevOps umbrella SRE Practices - Identify and measure SLIs, define SLOs and agree/ commit to SLA for product and service - Chaos Engineering - Removing toil - System designing (DR, Multi-Region, Mult-Cloud) - Postmortems/ Root Cause Analysis - Observability Platform Engineering Before jumping to definition, let’s understand the problem… “The composition and integration of a set of processes, tools and automation (components) to build a coherent platform with the goal of empowering developers to be able to easily build, maintain and deploy their business logic” Carrier as a DevOps Engineer CI/ CD Management & Automation Writing Specifications and Documentation Infrastructure Management Cloud Deployment and Management Performance Assessment and Monitoring DevOps Engineer Role Assisting with DevOps culture apdotion References • https://sre.google/sre-book/table-of-contents/ • https://www.gartner.com/en/articles/what-is-platform-engineering • https://youtu.be/uTEL8Ff1Zvk?si=5QT_LrzedX-BMezt Thank You! X (Twitter) https://www.linkedin.com/in/ravindufernando/ LinkedIn @ravindunf', 'Intro to DevOps and Beyond Ravindu Nirmal Fernando About Me • STL - DevOps @ Sysco LABS - Sri Lanka • MSc in Computer Science specialized in Cloud Computing (UOM) • AWS Certified Solutions Architect - Professional • Certified Kubernetes Administrator (CKA) • AWS Community Builder Ravindu Nirmal Fernando https://ravindunfernando.com The Era before DevOps Developers Focused on Agility Operators Focused on Stability Act 01 - Operations teams maintaining large fragile applications Doesn\\'t have any visibility on the application, whether or not its working as expected Act 03 - The Developers Developers taking shortcuts and putting more and more fragile code on top of existing ones Act 02 - The product managers Larger, unrealistic commitments made to the outside world (client/ investors) without understanding the complexities behind development and operations Act 04 - Dev and Ops at war \"It worked on my machine\" phenomenon \"Destructive downward spiral in IT\" - Gene Kim How can we overcome these issues? “DevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity” - What is DevOps? [AWS] - “A compound of development (Dev) and operations (Ops), DevOps is the union of people, process, and technology to continually provide value to customers.” - What is DevOps? [Azure] - DevOps allows evolving and improving products at a faster pace than businesses using traditional software development and infrastructure management processes. This speed allows businesses to serve their customers better and compete effectively. Reduce Organizational Silos Everyones shares the ownership of production and information is shared among everyone Measure Everything Application, systems monitoring and metrics etc... Implement Gradual Changes Frequent deployments, frequent deterministic releases in small chunks which can be rolled back Accept Failure as Normal Blameless PMs/ RCA. Risk taking mindset. Leverage Tooling and Automation Automate and reduce manual work as much as possible Key Areas in DevOps DevOps Practices • Continuous Integration (CI) - Software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. • Continuous Delivery (CD) - Software development practice where code changes are automatically built, tested, and prepared for a release to production (automated code change deployment to staging/ pre-production system). • Continuous Deployment (CD) - Every change that passes all stages of the pipeline will be deployed into production (released to customers). This practice fully automates the whole release flow without human intervention and only a failed test will prevent a new change being deployed. • Microservices - The microservices architecture is a design approach to build a single application as a set of small services with each focusing on SRP. Each service can be created, deployed and run independently. • Infrastructure as Code - A practice in which infrastructure is provisioned and managed using code and software development techniques, such as version control and continuous integration. ⚬Configuration Management ⚬Policy as Code • GitOps - builds on the concept of IaC, incorporating the functionality of Git repositories, merge', 'requests (MRs) and CI/CD to further unify software development and infrastructure operations. GitOps incorporates managing both infrastructure and applications as code. • Cloud Infrastructure - Cloud provides more flexibility, scalability and toolsets for organizations to implement DevOps culture and practices. Serverless architecture in cloud brings down the efforts of DevOps teams as it eliminates server management operations. • Continuous Monitoring, Logging and Alerting - Organizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product’s end user. Combined with real time alerts organizations can do a real time analysis on the application status. DevOps Tools and Technologies Beyond DevOps \"the practice of integrating security into a continuous integration, continuous delivery, and continuous deployment pipeline\" DevSecOps Idea of moving Security in the early stages of the SDLC pipeline SRE (Site Reliability Engineering) ⚬Not competing with DevOps ⚬Think that Class SRE implements Interface DevOps ⚬SRE is a part of the DevOps umbrella SRE Practices - Identify and measure SLIs, define SLOs and agree/ commit to SLA for product and service - Chaos Engineering - Removing toil - System designing (DR, Multi-Region, Mult-Cloud) - Postmortems/ Root Cause Analysis - Observability Platform Engineering Before jumping to definition, let’s understand the problem… “The composition and integration of a set of processes, tools and automation (components) to build a coherent platform with the goal of empowering developers to be able to easily build, maintain and deploy their business logic” Carrier as a DevOps Engineer CI/ CD Management & Automation Writing Specifications and Documentation Infrastructure Management Cloud Deployment and Management Performance Assessment and Monitoring DevOps Engineer Role Assisting with DevOps culture apdotion References • https://sre.google/sre-book/table-of-contents/ • https://www.gartner.com/en/articles/what-is-platform-engineering • https://youtu.be/uTEL8Ff1Zvk?si=5QT_LrzedX-BMezt Thank You! X (Twitter) https://www.linkedin.com/in/ravindufernando/ LinkedIn @ravindunf']\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "question = \"What is DevOps?\"\n",
    "answer, sources = generate_answer(question)\n",
    "\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"\\nSource Chunks:\\n\", sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
