{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook loads the local Mistral-7B model, performs retrieval\n",
    "from FAISS, and runs a RAG pipeline to answer user questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "MODEL_PATH = \"../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "VECTOR_INDEX_PATH = \"../vectorstore/ctse_faiss.index\"\n",
    "CHUNKS_PATH = \"../vectorstore/chunks.txt\"\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load LLM ---\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6, n_gpu_layers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load vector index and chunks ---\n",
    "index = faiss.read_index(VECTOR_INDEX_PATH)\n",
    "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "    chunks = f.read().split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG Pipeline ---\n",
    "def retrieve_chunks(query, k=TOP_K):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    query_vec = embedder.encode([query])\n",
    "    D, I = index.search(np.array(query_vec).astype('float32'), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def generate_answer(query):\n",
    "    context_chunks = retrieve_chunks(query)\n",
    "    context = \"\\n---\\n\".join(context_chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant helping students learn about Current Trends in Software Engineering (CTSE).\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, stop=[\"\\n\\n\"])\n",
    "    return response['choices'][0]['text'].strip(), context_chunks\n",
    "\n",
    "# --- Example Usage ---\n",
    "# question = \"What is the role of DevOps in modern software engineering?\"\n",
    "# answer, sources = generate_answer(question)\n",
    "# print(\"Answer:\\n\", answer)\n",
    "# print(\"\\nSource Chunks:\\n\", sources)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
