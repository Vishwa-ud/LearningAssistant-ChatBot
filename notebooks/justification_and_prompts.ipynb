{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# justification_and_prompts.ipynb\n",
    "\n",
    "\"\"\"\n",
    "This notebook documents:\n",
    "- Justification for LLM choice (Mistral)\n",
    "- Justification for development approach (RAG, Chainlit)\n",
    "- Transparency in GenAI prompt usage\n",
    "\"\"\"\n",
    "\n",
    "# --- ğŸ“Œ LLM Choice Justification ---\n",
    "\"\"\"\n",
    "I chose `mistral-7b-instruct-v0.1.Q4_K_M.gguf` for the following reasons:\n",
    "\n",
    "- âœ… Open-source and free to use â€” avoids costs associated with closed LLMs (e.g., ChatGPT)\n",
    "- âœ… Lightweight quantized model (`Q4_K_M`) suitable for local inference\n",
    "- âœ… Instruction-tuned, making it better at following prompts and engaging in QA\n",
    "- âœ… Good performance for academic-level questions with reasonable context\n",
    "\"\"\"\n",
    "\n",
    "# --- ğŸ› ï¸ Development Approach Justification ---\n",
    "\"\"\"\n",
    "I adopted the Retrieval-Augmented Generation (RAG) architecture because:\n",
    "\n",
    "- ğŸ” It grounds the language model's output on course-specific content (e.g., CTSE slides/notes)\n",
    "- ğŸ“š Makes answers more factual, reduces hallucinations\n",
    "- ğŸ” Supports document updates without fine-tuning the model\n",
    "- ğŸ§  Embedding + vector search ensures semantic similarity, not just keyword matches\n",
    "\n",
    "For UI, I used **Chainlit** because:\n",
    "- ğŸ§ª Interactive chat interface optimized for LLM workflows\n",
    "- ğŸ“‚ Supports file upload (PDF, PPTX) easily\n",
    "- ğŸ§° Seamless integration with Python-based backends (LLM + RAG)\n",
    "\"\"\"\n",
    "\n",
    "# --- ğŸ’¬ GenAI Prompts Used + Reflections ---\n",
    "\"\"\"\n",
    "Prompt for answering a user question:\n",
    "\n",
    "```\n",
    "You are an expert assistant helping students learn about Current Trends in Software Engineering (CTSE).\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\n",
    "```\n",
    "\n",
    "Reflection:\n",
    "- ğŸ“Œ This prompt is instructional and role-based (\"expert assistant\"), which helps steer model behavior\n",
    "- ğŸ“‘ It includes a context window that keeps the model grounded in retrieved data\n",
    "- ğŸ§ª Stop sequences were added to avoid overly long or irrelevant output\n",
    "\n",
    "Other prompts used:\n",
    "- N/A (all logic is handled in code)\n",
    "\"\"\"\n",
    "\n",
    "# --- âœ… Summary ---\n",
    "\"\"\"\n",
    "- Mistral is chosen for accessibility, performance, and cost-efficiency\n",
    "- RAG is chosen for accurate, document-based QA\n",
    "- Chainlit enhances the UX while supporting fast prototyping\n",
    "- All prompt usage is logged and explained\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
